{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similaridade com GENSIM\n",
    "======================\n",
    "\n",
    "O notebook em questão demonstra como buscar similaridade por exemplo de uma texto em um conjunto de documentos.\n",
    "Utilizaremos para isto a biblioteca Python denominada Gensim. Esta biblioteca é utilizadano processamento de linguagem natural (NLP), em funções como indexação de documentos e análise de similaridade entre textos.\n",
    "\n",
    "Os principais conceitos utilizados em gensim são:\n",
    "\n",
    "    Document(o): algum texto.\n",
    "\n",
    "    Corpus: uma coleção de documentos.\n",
    "\n",
    "    Vector: uma representação matemática de um documento.\n",
    "\n",
    "    Model(o): an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "A seguir o passo a passo para efetuarmos análise de Simularidade pelo Gensim.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o Corpus\n",
    "-------------------\n",
    "\n",
    "Antes de qualquer coisa precisamos criar um Corpus.\n",
    "Nos documentos originais existem palavras que não contribuem para a análise semântica, \n",
    "por exemplo, artigos e preposições. Estas palavras serão removidas do nosso corpus. \n",
    "Além disso realizaremos o processo de \"steeming\", ou seja, o processo de reduzir\n",
    "palavras flexionadas (ou às vezes derivadas) ao seu tronco (stem) base ou raiz, \n",
    "geralmente uma forma da palavra escrita. Ex: Devolver e Devolverei se tranformam em devolv\n",
    "Além do stemming faremos o processo de tokenização, processo de converter uma sequência de \n",
    "caracteres em uma sequência de tokens (no nosso caso palavras).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['qual', 'statu', 'ped'], ['ped', 'est', 'statu'], ['com', 'acion', 'vend'], ['qual', 'vend', 'pod', 'atend'], ['com', 'devolv', 'produt'], ['devolv', 'produt', 'ped'], ['quer', 'faz', 'devoluç', 'produt'], ['tenh', 'reclam'], ['quer', 'faz', 'reclam'], ['dia', 'ser', 'entreg', 'produt'], ['qual', 'praz', 'entreg'], ['qual', 'dat', 'entreg'], ['qual', 'catálag', 'produt'], ['com', 'acess', 'catálag', 'produt'], ['quer', 'devolv', 'ped']]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "#from nltk.stem import PorterStemmer /*Stemmer para Inglês*/\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "#Comando para importar o Stemmer e stopwords do idioma Português\n",
    "#import nltk\n",
    "#nltk.download('rslp') \n",
    "#nltk.download('stopwords')\n",
    "\n",
    "ps = RSLPStemmer() #Portuguese Stemmer\n",
    "# Esta é nossa lista de documentos original,\n",
    "# pode ser por exemplo todas as perguntas que podem ser feitas em um chatbot\n",
    "documentos = [\n",
    "    \"Qual o status do meu pedido\",\n",
    "    \"Meu pedido está em que status\",\n",
    "    \"Como acionar um vendedor\",\n",
    "    \"Qual vendedor pode me atender\",\n",
    "    \"Como devolver o produto\",\n",
    "    \"Devolverei um produto e pedido\",\n",
    "    \"Quero fazer uma devolução de produto\",\n",
    "    \"Tenho uma reclamação\",\n",
    "    \"Quero fazer uma reclamação\",\n",
    "    \"em que dia será entregue meu produto\",\n",
    "    \"Qual o prazo de entrega\",\n",
    "    \"Qual a data de entrega\",\n",
    "    \"Qual o catálago de produtos\",\n",
    "    \"Como acesso o catálago de produtos\",\n",
    "    \"Quero devolver o pedido\"]\n",
    "\n",
    "\n",
    "stoplist = set('o a em de do da uma um meu minha me te e que'.split())\n",
    "#Criamos nossa própria lista de stopwords, mas se quisesse utilizar todas as stopwords do portugues poderia\n",
    "#ter feito stoplist = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "texts = [\n",
    "    [ps.stem(word) for word in documento.lower().split() if word not in stoplist]\n",
    "    for documento in documentos]\n",
    "print(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeremos as palavras que aparecem apenas uma vez\n",
    "frequency = defaultdict(int) \n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nosso dicionário possui apenas 12 palavras, 12 tokens únicos, abaixo listados com seus respectivos códigos:\n",
      "{'ped': 0, 'qual': 1, 'statu': 2, 'com': 3, 'vend': 4, 'devolv': 5, 'produt': 6, 'faz': 7, 'quer': 8, 'reclam': 9, 'entreg': 10, 'catálag': 11}\n",
      "Nosso corpus reflete nossos documentos, conforme este dicionário, em um formato de vetor, onde para cada documento se tem o código do token e o número de vezes que ele ocorre no corpus como um todo.\n",
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (2, 1)], [(3, 1), (4, 1)], [(1, 1), (4, 1)], [(3, 1), (5, 1), (6, 1)], [(0, 1), (5, 1), (6, 1)], [(6, 1), (7, 1), (8, 1)], [(9, 1)], [(7, 1), (8, 1), (9, 1)], [(6, 1), (10, 1)], [(1, 1), (10, 1)], [(1, 1), (10, 1)], [(1, 1), (6, 1), (11, 1)], [(3, 1), (6, 1), (11, 1)], [(0, 1), (5, 1), (8, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print('Nosso dicionário possui apenas 12 palavras, 12 tokens únicos, abaixo listados com seus respectivos códigos:')\n",
    "print (dictionary.token2id)\n",
    "print ('Nosso corpus reflete nossos documentos, conforme este dicionário, em um formato de vetor, onde para cada documento se tem o código do token e o número de vezes que ele ocorre no corpus como um todo.')\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o modelo de Similaridade LSI\n",
    "--------------------\n",
    "\n",
    "LSI é um algorítimo de NLP que relaciona de forma semântica um conjunto de documentos e os termos neles contidos. Esta técnica assume que palavras que são próximas em significado vão ocorrer de forma similar em diferentes trechos do texto.\n",
    "Portanto cria-se uma matriz (vetor) contendo o número de vezes que a palavra (token) apareceu no documento. Documentos são comparados utilizando o cosseno do ângulo dos dois vetores, valores próximo a 1 representam alta similaridaden próximos de zero baixa similaridade.\n",
    "Abaixo criaremos nosso modelo LSI tendo como base nosso corpus gerado, com seu respectivo dicionário, para geração do vetor LSI espacial com 10 dimensões, que são o número de topicos que passamos como parâmetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models \n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['devolv', 'produt']\n",
      "[(5, 1), (6, 1)]\n",
      "[(0, -0.9882633035409889), (1, -0.4602645609857196), (2, -0.13891911633974807), (3, 0.30152438285289773), (4, 0.3855089709829175), (5, -0.30604202150079923), (6, 0.2593872054695901), (7, 0.15575861299784527), (8, -0.412755613467044), (9, 0.2615849681569986)]\n"
     ]
    }
   ],
   "source": [
    "#Temos nosso modelo LSI gerado, suponhamos que temos um novo documento, no caso a string doc e queremos verificar como \n",
    "#esta string se relaciona como nosso Corpus anterior. Quais documentos são semelhantes semanticamente? Para responder isto utilizaremos\n",
    "#o modelo LSI gerado\n",
    "docnovo = \"Devolver produto\" #novo documento\n",
    "doc = [ps.stem(word) for word in docnovo.split()] #fazemos o stemming deste novo documento bem como sua tokenização\n",
    "vec_bow = dictionary.doc2bow(doc)    \n",
    "vec_lsi = lsi[vec_bow]  # converte a query para um vector LSI\n",
    "print(doc)\n",
    "print(vec_bow)\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.009093601), (1, -0.018258948), (2, 0.009364074), (3, -0.011369765), (4, 0.8320902), (5, 0.86795425), (6, 0.4280177), (7, 0.0019537657), (8, -0.0015561131), (9, 0.5057455), (10, 0.0061714984), (11, 0.0061714984), (12, 0.4133898), (13, 0.43031746), (14, 0.41526675)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])  # transforma o corpus original para um vetor espacial LSI e indexa o mesmo\n",
    "sims = index[vec_lsi]  # executa a query de similaridade do doc novo contra o corpus original\n",
    "print(list(enumerate(sims)))  # imprime (numero documento corpus original, valor de similaridade) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Devolver produto \" se relaciona de mais para menos com os seguintes documentos:\n",
      "5 0.86795425 Devolverei um produto e pedido\n",
      "4 0.8320902 Como devolver o produto\n",
      "9 0.5057455 em que dia será entregue meu produto\n",
      "13 0.43031746 Como acesso o catálago de produtos\n",
      "6 0.4280177 Quero fazer uma devolução de produto\n",
      "14 0.41526675 Quero devolver o pedido\n",
      "12 0.4133898 Qual o catálago de produtos\n",
      "2 0.009364074 Como acionar um vendedor\n",
      "0 0.009093601 Qual o status do meu pedido\n",
      "10 0.0061714984 Qual o prazo de entrega\n",
      "11 0.0061714984 Qual a data de entrega\n",
      "7 0.0019537657 Tenho uma reclamação\n",
      "8 -0.0015561131 Quero fazer uma reclamação\n",
      "3 -0.011369765 Qual vendedor pode me atender\n",
      "1 -0.018258948 Meu pedido está em que status\n"
     ]
    }
   ],
   "source": [
    "#Verificando e imprimindo em ordem descendente o id do documento original, o valor de similaridade e o texto original\n",
    "#do documento. Uma visão que mais se relaciona para o que menos se relaciona ao nosso novo documento.\n",
    "def take_second(elem):\n",
    "    return elem[1]\n",
    "\n",
    "sims2 = sorted(list(enumerate(sims)),key=take_second,reverse=True)\n",
    "#print(sims2)\n",
    "print('\"',docnovo,'\" se relaciona de mais para menos com os seguintes documentos:')\n",
    "\n",
    "j = (list(enumerate(sims2))[-1][0])  #número total de tuplas\n",
    "i = 0\n",
    "while i <= j:\n",
    "    t = sims2[i][0] #indice do documento original\n",
    "    print(t,sims2[i][1],documentos[t])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referências\n",
    "https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing\n",
    "http://www.nltk.org/howto/portuguese_en.html\n",
    "https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html#sphx-glr-auto-examples-core-run-similarity-queries-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
